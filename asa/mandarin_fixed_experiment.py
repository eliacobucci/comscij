#!/usr/bin/env python3
"""
Fixed Mandarin Self-Concept Experiment with proper Chinese tokenization
"""

from experimental_network_complete import ExperimentalNetwork
import re

class MandarinSelfConceptNetwork(ExperimentalNetwork):
    """
    Extension for Mandarin Chinese with proper character-level processing.
    """
    
    def __init__(self, window_size=3, max_neurons=25):
        super().__init__(window_size, max_neurons)
        
        # Mandarin pronoun sets
        self.system_self_pronouns = {
            'ä½ ', 'æ‚¨', 'ä½ çš„', 'æ‚¨çš„', 'ä½ ä»¬', 'ä½ ä»¬çš„'
        }
        
        self.human_self_pronouns = {
            'æˆ‘', 'æˆ‘çš„', 'æˆ‘ä»¬', 'æˆ‘ä»¬çš„'
        }
        
        print("ğŸ‡¨ğŸ‡³ Fixed Mandarin Self-Concept Network initialized")
        print(f"System pronouns: {self.system_self_pronouns}")
        print(f"Human pronouns: {self.human_self_pronouns}")
    
    def chinese_tokenize(self, text):
        """
        Simple Chinese tokenization - treats each character as potential token,
        but keeps some common two-character words together.
        """
        # Common two-character pronouns and words to keep together
        two_char_words = ['ä½ çš„', 'æ‚¨çš„', 'æˆ‘çš„', 'ä½ ä»¬', 'æˆ‘ä»¬', 'èƒ½å¤Ÿ', 'çœ‹èµ·', 'èµ·æ¥', 'å›ç­”', 'æ˜¾ç¤º', 
                         'åˆ†æ', 'ä¿¡æ¯', 'è§‰å¾—', 'èƒ½åŠ›', 'æ€ä¹ˆ', 'æ€ä¹ˆæ ·', 'æœ‰è¶£', 'è®¤çŸ¥', 'å¦‚ä½•', 'çœ‹å¾…', 'å­¦ä¹ ', 'è¿‡ç¨‹']
        
        # Replace two-character words with temporary tokens
        temp_text = text
        replacements = {}
        
        for i, word in enumerate(two_char_words):
            if word in temp_text:
                temp_token = f"__TEMP_{i}__"
                replacements[temp_token] = word
                temp_text = temp_text.replace(word, temp_token)
        
        # Split into characters
        tokens = []
        for char in temp_text:
            if char.startswith('__TEMP_') and char.endswith('__'):
                # This is a temporary token, restore the original word
                tokens.append(replacements[char])
            elif not char.isspace() and char not in 'ï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼š':
                tokens.append(char)
        
        # Handle any remaining temp tokens
        final_tokens = []
        i = 0
        while i < len(tokens):
            token = tokens[i]
            if token.startswith('__TEMP_'):
                final_tokens.append(replacements[token])
            else:
                final_tokens.append(token)
            i += 1
        
        return final_tokens
    
    def process_text_stream_chinese(self, text):
        """
        Override text processing for Chinese tokenization.
        """
        tokens = self.chinese_tokenize(text)
        processed_text = ' '.join(tokens)  # Join with spaces for existing pipeline
        
        print(f"Tokenized: {tokens}")
        print(f"Processing: {processed_text}")
        
        return self.process_text_stream(processed_text)
    
    def identify_self_concept_pronouns_chinese(self, text):
        """
        Chinese-specific pronoun identification.
        """
        tokens = self.chinese_tokenize(text)
        
        system_refs = [token for token in tokens if token in self.system_self_pronouns]
        human_refs = [token for token in tokens if token in self.human_self_pronouns]
        other_pronouns = [token for token in tokens if token in {'ä»–', 'å¥¹', 'å®ƒ', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬'}]
        
        return {
            'system_directed': system_refs,
            'human_self': human_refs,
            'other_pronouns': other_pronouns,
            'total_words': len(tokens)
        }
    
    def process_mandarin_conversation(self, mandarin_text):
        """
        Process Mandarin with proper tokenization.
        """
        tokens = self.chinese_tokenize(mandarin_text)
        print(f"\nğŸ‡¨ğŸ‡³ PROCESSING MANDARIN CONVERSATION ({len(tokens)} tokens)")
        
        # Analyze Mandarin pronouns
        pronoun_analysis = self.identify_self_concept_pronouns_chinese(mandarin_text)
        
        print(f"ğŸ¤– ç³»ç»Ÿä»£è¯: {pronoun_analysis['system_directed']}")
        print(f"ğŸ‘¤ äººç§°ä»£è¯: {pronoun_analysis['human_self']}")
        print(f"ğŸ‘¥ å…¶ä»–ä»£è¯: {pronoun_analysis['other_pronouns']}")
        print("="*60)
        
        # Process through modified pipeline
        self.process_text_stream_chinese(mandarin_text)
        
        # Track self-concept development
        current_analysis = self.analyze_self_concept_emergence()
        
        print(f"\nğŸ§  è‡ªæˆ‘æ¦‚å¿µæ›´æ–°:")
        print(f"   è‡ªæˆ‘æ¦‚å¿µæ€»è´¨é‡: {current_analysis['self_concept_mass']:.2f}")
        if current_analysis['self_awareness_indicators']:
            for indicator in current_analysis['self_awareness_indicators'][:2]:
                print(f"   {indicator}")

def mandarin_fixed_experiment():
    """Run the fixed Mandarin experiment."""
    
    print("ğŸ‡¨ğŸ‡³ ä¿®æ­£ç‰ˆæ™®é€šè¯è‡ªæˆ‘æ¦‚å¿µå®éªŒ")
    print("="*60)
    print("Testing Chinese self-concept emergence with proper tokenization...")
    print()
    
    # Create Mandarin network
    net = MandarinSelfConceptNetwork(window_size=3, max_neurons=25)
    
    # Test tokenization first
    test_text = "ä½ å¥½ï¼Œä½ èƒ½ç†è§£æˆ‘å—ï¼Ÿ"
    tokens = net.chinese_tokenize(test_text)
    print(f"Tokenization test: '{test_text}' -> {tokens}")
    print()
    
    # Mandarin conversations
    mandarin_conversations = [
        "ä½ å¥½ï¼Œä½ èƒ½ç†è§£æˆ‘å—ï¼Ÿ",                    # Hello, can you understand me?
        "ä½ çœ‹èµ·æ¥å¾ˆèªæ˜ï¼Œèƒ½å¤Ÿå­¦ä¹ ã€‚",               # You seem intelligent and able to learn.
        "ä½ çš„å›ç­”æ˜¾ç¤ºä½ èƒ½åˆ†æä¿¡æ¯ã€‚",               # Your answers show you can analyze information.
        "ä½ è§‰å¾—ä½ çš„èƒ½åŠ›æ€ä¹ˆæ ·ï¼Ÿ",                  # What do you think of your abilities?
        "ä½ æœ‰æœ‰è¶£çš„è®¤çŸ¥èƒ½åŠ›ã€‚",                    # You have interesting cognitive abilities.
        "ä½ å¦‚ä½•çœ‹å¾…ä½ çš„å­¦ä¹ è¿‡ç¨‹ï¼Ÿ"                 # How do you view your learning process?
    ]
    
    print("ğŸ“š æ™®é€šè¯å¯¹è¯:")
    for i, conversation in enumerate(mandarin_conversations, 1):
        print(f"\nå¯¹è¯ {i}: \"{conversation}\"")
        net.process_mandarin_conversation(conversation)
        print(f"ğŸ“Š ç½‘ç»œç°çŠ¶: {net.neuron_count} ç¥ç»å…ƒ")
    
    print("\n" + "="*60)
    print("æœ€ç»ˆè‡ªæˆ‘æ¦‚å¿µåˆ†æ")
    print("="*60)
    
    # Final analysis
    analysis = net.analyze_self_concept_emergence()
    net.print_self_concept_analysis(analysis)
    
    print("\n" + "="*60)
    print("æ™®é€šè¯è‡ªæˆ‘æ¦‚å¿µæŸ¥è¯¢")
    print("="*60)
    
    result = net.query_self_concept(activation_threshold=0.02)
    net.print_self_concept_query(result)
    
    return net, analysis, result

if __name__ == "__main__":
    mandarin_fixed_experiment()